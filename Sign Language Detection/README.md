
# Sign Language Detection using Action Recognition

Sign Language Detection is a project that aims to recognize and interpret hand gestures from sign language. This comprehensive solution uses the LSTM model to identify sign language phrases like "Hello," "I love you," "father," "bye," and "mother." The model can close communication gaps and foster better understanding between individuals who use sign language and those who do not.

## Dataset
The dataset for the project is created with **mediapipe**. MediaPipe is an open-source library developed by Google that provides a comprehensive set of tools for building various types of perception and machine learning applications. It is designed to simplify the development of computer vision and media processing pipelines by offering reusable and efficient components.

## Features

- **LSTM Model:** Utilizes Long Short-Term Memory networks for capturing temporal dependencies in sign language gestures.
- **MediaPipe:** Integrates the MediaPipe library for hand tracking and keypoint extraction, enhancing the model's ability to interpret sign language accurately.
- **Real-time Detection:** The system provides real-time sign language detection, making it suitable for interactive applications.

## Requirements
Install the required dependencies by running:\
pip install -r requirements.txt

## Results
The LSTM model achieves real-time sign language detection with high accuracy in various scenarios.

## Conclusion
The project of Sign Language Detection using LSTM is a promising initiative that aims to empower individuals who use sign language to communicate with the wider community. By utilizing the potential of deep learning and computer vision, this project strives to make sign language more accessible and inclusive for everyone.